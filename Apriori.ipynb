{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apriori.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnLNHaP23rPMBElnqDBzz2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-Kamtekar/Optional_Assignment/blob/main/Apriori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First import the libraries which will be useful."
      ],
      "metadata": {
        "id": "8XO520lwVmHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from itertools import combinations\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "K9gbX7yuVoJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mounting drive"
      ],
      "metadata": {
        "id": "J5q6vB_xWCqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf8rNBKiWDAF",
        "outputId": "73a2f628-1958-4065-e18c-76466be53d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the data from csv file and store in dict."
      ],
      "metadata": {
        "id": "82U5kSFUVvvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_loc='/content/drive/MyDrive/255/Optional_Assignment_1/groceries.csv'):\n",
        "    trans = dict()\n",
        "    with open(file_loc) as f:\n",
        "        filedata = csv.reader(f, delimiter=',')\n",
        "        count = 0\n",
        "        for line in filedata:\n",
        "            count += 1\n",
        "            trans[count] = list(set(line))\n",
        "    return trans"
      ],
      "metadata": {
        "id": "FuuavMRWVwRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count the frequency of each item."
      ],
      "metadata": {
        "id": "tBma-ZWKWYXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def frequence(items_lst, trans, check=False):\n",
        "    items_counts = dict()\n",
        "    for i in items_lst:\n",
        "        temp_i = {i}\n",
        "        if check:\n",
        "            temp_i = set(i)\n",
        "        for j in trans.items():\n",
        "            if temp_i.issubset(set(j[1])):\n",
        "                if i in items_counts:\n",
        "                    items_counts[i] += 1\n",
        "                else:\n",
        "                    items_counts[i] = 1\n",
        "    return items_counts"
      ],
      "metadata": {
        "id": "R-mQFIolWY19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Form association rules form support."
      ],
      "metadata": {
        "id": "5sKWgT2RWbbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def association_rules(items_grater_then_min_support):\n",
        "    rules = []\n",
        "    dict_rules = {}\n",
        "    for i in items_grater_then_min_support:\n",
        "        dict_rules = {}\n",
        "        if type(i) != type(str()):\n",
        "            i = list(i)\n",
        "            temp_i = i[:]\n",
        "            for j in range(len(i)):\n",
        "                k = temp_i[j]\n",
        "                del temp_i[j]\n",
        "                dict_rules[k] = temp_i\n",
        "                temp_i = i[:]\n",
        "        rules.append(dict_rules)\n",
        "    temp = []\n",
        "    for i in rules:\n",
        "        for j in i.items():\n",
        "            if type(j[1]) != type(str()):\n",
        "                temp.append({tuple(j[1])[0]: j[0]})\n",
        "            else:\n",
        "                temp.append({j[1]: j[0]})\n",
        "    rules.extend(temp)\n",
        "    return rules"
      ],
      "metadata": {
        "id": "mYEWZCZ2WbjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the confidence of those association rules and take only rules which are greater than the minimum confidence."
      ],
      "metadata": {
        "id": "Uaebltm7WihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confidence(associations, d, min_confidence):\n",
        "    ans = {}\n",
        "    for i in associations:\n",
        "        for j in i.items():\n",
        "            if type(j[0]) == type(str()):\n",
        "                left = {j[0]}\n",
        "            else:\n",
        "                left = set(j[0])\n",
        "            if type(j[1]) == type(str()):\n",
        "                right = {j[1]}\n",
        "            else:\n",
        "                right = set(j[1])\n",
        "\n",
        "            for k in d:\n",
        "                if type(k) != type(str()):\n",
        "                    if left.union(right) - set(k) == set():\n",
        "                        up = d[k]\n",
        "                    if len(right) == len(set(k)) and right - set(k) == set():\n",
        "                        down = d[k]\n",
        "                else:\n",
        "                    if len(right) >= len({k}):\n",
        "                        if right - {k} == set():\n",
        "                            down = d[k]\n",
        "                        elif len(right) <= len({k}):\n",
        "                            if {k} - right == set():    \n",
        "                                down = d[k]\n",
        "            if up/down >= min_confidence:\n",
        "                ans[tuple(left)[0]] = right, up/down, up, down\n",
        "    return ans"
      ],
      "metadata": {
        "id": "r_ZYsLO5WinG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here is our main function that operates above code and here you can change the minimum support and confidence."
      ],
      "metadata": {
        "id": "VvYLJLI3WnNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(min_support, min_confidence, file_loc):\n",
        "    trans = read_data()\n",
        "    number_of_trans = [len(i) for i in trans.values()]\n",
        "    items_lst = set()\n",
        "\n",
        "    itemcount_track = list()\n",
        "    \n",
        "    for i in trans.values():\n",
        "        for j in i:\n",
        "            items_lst.add(j)\n",
        "\n",
        "    store_item_lst = list(items_lst)[:]\n",
        "    items_grater_then_min_support = list()\n",
        "    items_counts = frequence(items_lst, trans)\n",
        "    itemcount_track.append(items_counts)\n",
        "    items_grater_then_min_support.append({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support})\n",
        "\n",
        "    for i in range(2, max(number_of_trans)+1):\n",
        "        item_list = combinations(items_lst, i)\n",
        "        items_counts = frequence(item_list, trans, check=True)\n",
        "        itemcount_track.append(items_counts)\n",
        "        if list({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support}.keys()) != []:\n",
        "            items_grater_then_min_support.append({j[0]:j[1] for j in support(items_counts, trans).items() if j[1]>min_support})\n",
        "\n",
        "    d = {}\n",
        "    {d.update(i) for i in itemcount_track}\n",
        "    associations = association_rules(items_grater_then_min_support[len(items_grater_then_min_support)-1])\n",
        "    associations_grater_then_confidene = confidence(associations, d, min_confidence)\n",
        "\n",
        "    print(associations_grater_then_confidene)\n",
        "\n",
        "main(0.01, 0.7, '/content/drive/MyDrive/255/Optional_Assignment_1/groceries.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "nkqDoZsTWnYq",
        "outputId": "8a8a139e-21d8-4351-d06a-0b8f5d565bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6d16de4c932f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massociations_grater_then_confidene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GroceryStoreDataSet.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6d16de4c932f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(min_support, min_confidence, file_loc)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mitems_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mitemcount_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mitems_grater_then_min_support\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mmin_support\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'support' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "\n",
        "transactions = {\n",
        "    1: [\"1\", \"3\", \"4\"],\n",
        "    2: [\"2\", \"3\", \"5\"],\n",
        "    3: [\"1\", \"2\", \"3\", \"5\"],\n",
        "    5: [\"2\", \"5\"],\n",
        "    6: [\"1\", \"3\", \"5\"]\n",
        "}\n",
        "\n",
        "min_support_count = 2\n",
        "min_confidence_value = 0.6\n",
        "\n",
        "\n",
        "# apriori pruning concept\n",
        "def _pruning(current, previous, size):\n",
        "    final_keys = []\n",
        "    previous = [tuple(i) for i in previous]\n",
        "    for key in current:\n",
        "        FLAG = False\n",
        "        current_comb = list(combinations(key, size))\n",
        "        for i in current_comb:\n",
        "            if i in previous or i[::-1] in previous:\n",
        "                FLAG = True\n",
        "            else:\n",
        "                FLAG = False\n",
        "                break\n",
        "\n",
        "        if FLAG:\n",
        "            final_keys.append(key)\n",
        "\n",
        "    return final_keys\n",
        "\n",
        "\n",
        "def support_value(itemset_keys_, transactions):\n",
        "    itemset = {key: 0 for key in itemset_keys_}\n",
        "\n",
        "    for keys in itemset_keys_:\n",
        "        for val in transactions.values():\n",
        "            if set(keys) & set(val) == set(keys):\n",
        "                itemset[keys] += 1\n",
        "    return itemset\n",
        "\n",
        "\n",
        "# creating frequent itemset\n",
        "def get_frequent_itemset(size=None, transactions=None, itemset=None):\n",
        "    if size == 1:\n",
        "        itemset = Counter()\n",
        "\n",
        "        for val in transactions.values():\n",
        "            itemset.update(val)\n",
        "\n",
        "    else:\n",
        "\n",
        "        prev_itemset_keys = list(itemset.keys())\n",
        "        prev_itemset = itemset.copy()\n",
        "\n",
        "        valid_keys = list(set(itemset.keys()))\n",
        "        # flatten list of tuple -> keys: [(), ()] -> []\n",
        "        # useful for running a combination of all the chosen features\n",
        "        l = []\n",
        "        for row in valid_keys:\n",
        "            l.extend(row)\n",
        "\n",
        "        valid_keys = set(l)\n",
        "\n",
        "        # candidate itemset keys\n",
        "        itemset_keys_ = list(combinations(valid_keys, size))\n",
        "\n",
        "        # Apriori algorithm is based on theconcept that a subset\n",
        "        # of a frequent itemset must also be a frequent itemset\n",
        "        # so we are pruning away those features whose subset are not present\n",
        "        # in the previous frequent itemset\n",
        "        if size >= 2:\n",
        "            itemset_keys_ = _pruning(\n",
        "                itemset_keys_, prev_itemset_keys, size - 1)\n",
        "\n",
        "        # finding support value for each of the selected itemset feature combination\n",
        "        itemset = support_value(itemset_keys_, transactions)\n",
        "\n",
        "        # defaulting back to th previous frequent itemset if\n",
        "        # the iteration doesn't find any itemset which has the theshold required\n",
        "        if itemset == {}:\n",
        "            itemset = prev_itemset\n",
        "\n",
        "    # getting frequent itemset from itemset\n",
        "    # Frequent Itemset is an itemset whose support\n",
        "    # value is greater than a threshold value(support).\n",
        "\n",
        "    frequent_itemset = {}\n",
        "    for key, val in itemset.items():\n",
        "        if val >= min_support_count:\n",
        "            frequent_itemset[key] = val\n",
        "\n",
        "    return frequent_itemset\n",
        "\n",
        "\n",
        "def finding_subsets(frequent_set):\n",
        "    item_list = []\n",
        "    size = len(list(frequent_set.keys())[0])\n",
        "    for key in frequent_set.keys():\n",
        "        subsets = []\n",
        "        for i in range(1, size):\n",
        "            subsets.append(list(combinations(key, i)))\n",
        "\n",
        "        subsets = list(np.array(subsets).flatten())\n",
        "        subsets.insert(0, key)\n",
        "        item_list.append(subsets)\n",
        "\n",
        "    return item_list\n",
        "\n",
        "\n",
        "def finding_rules(itemset_sub):\n",
        "    print(\"Antecedents -->  Consequents --- Confidence\")\n",
        "    for i in range(1, len(itemset_sub)):\n",
        "\n",
        "        # passing as list as we have designed support_value function as\n",
        "        # a function that takes an iteratable list of itemsets\n",
        "        x = support_value([itemset_sub[0], ], transactions)\n",
        "        y = support_value([itemset_sub[i], ], transactions)\n",
        "        confidence = list(x.values())[0] / list(y.values())[0]\n",
        "        if confidence >= min_confidence_value:\n",
        "            print(\n",
        "                f\"{itemset_sub[i]} --> {itemset_sub[0]} --- {round(confidence, 2)}\")\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        "    ITEMS\n",
        "    1: Banana\n",
        "    2: Eggs\n",
        "    3: Milk\n",
        "    4: Tea\n",
        "    5: Bread\n",
        "\"\"\")\n",
        "\n",
        "f = {}\n",
        "\n",
        "for i in range(1, 5):\n",
        "    f = get_frequent_itemset(size=i, transactions=transactions,\n",
        "                             itemset=f)\n",
        "\n",
        "# frequent_itemsets\n",
        "\n",
        "print(\"Frequent Itemsets...\")\n",
        "for key, val in f.items():\n",
        "    print(f\"Itemset: {key}, support value: {val}\")\n",
        "\n",
        "\n",
        "subset = finding_subsets(f)\n",
        "\n",
        "for i in subset:\n",
        "    print(f\"Rules for itemset - {i[0]}\")\n",
        "    finding_rules(i)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP0HMI3_X5kX",
        "outputId": "97d2b0cf-4a2c-475b-8191-c20b414dbd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    ITEMS\n",
            "    1: Banana\n",
            "    2: Eggs\n",
            "    3: Milk\n",
            "    4: Tea\n",
            "    5: Bread\n",
            "\n",
            "Frequent Itemsets...\n",
            "Itemset: ('3', '1', '5'), support value: 2\n",
            "Itemset: ('3', '2', '5'), support value: 2\n",
            "Rules for itemset - ('3', '1', '5')\n",
            "Antecedents -->  Consequents --- Confidence\n",
            "('1',) --> ('3', '1', '5') --- 0.67\n",
            "('3', '1') --> ('3', '1', '5') --- 0.67\n",
            "('3', '5') --> ('3', '1', '5') --- 0.67\n",
            "('1', '5') --> ('3', '1', '5') --- 1.0\n",
            "\n",
            "Rules for itemset - ('3', '2', '5')\n",
            "Antecedents -->  Consequents --- Confidence\n",
            "('2',) --> ('3', '2', '5') --- 0.67\n",
            "('3', '2') --> ('3', '2', '5') --- 1.0\n",
            "('3', '5') --> ('3', '2', '5') --- 0.67\n",
            "('2', '5') --> ('3', '2', '5') --- 0.67\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:120: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    }
  ]
}